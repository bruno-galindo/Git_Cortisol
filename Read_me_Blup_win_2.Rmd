---
title: "Read_me"
author: "Galindo, B. A"
date: "2024-05-22"
output:
  html_document:
    css: styles.css
    toc: true
    number_sections: true
  pdf_document:
    toc: true
---


# Raw data
```{r eval=FALSE}
data <- read.csv("/home/bambrozi/2_CORTISOL/Data/T4_Elora_Data_04_25_2024.csv")

# Replace "treated" with NA
data$T4Cortisol[data$T4Cortisol == "treated" | data$T4Cortisol == "Treated at T2" | data$T4Cortisol == "treated at T2"] <- NA
# Convert the column to numeric, coercing non-numeric values to NA
data$T4Cortisol <- as.numeric(as.character(data$T4Cortisol))
#Filtering only the lines with values
data <- data[!is.na(data$T4Cortisol),]
#creating new data file cleaned  
write.csv(data, "/home/bambrozi/2_CORTISOL/Data/data_clean.csv", row.names = F)

print(data)
```

# Continuous Phenotype

```{r eval=FALSE}
# Summary Statistics
summary(data$T4Cortisol)
# Histogram
hist(data$T4Cortisol, breaks = 20, main = "Histogram of T4 Cortisol", xlab = "T4 Cortisol")
# Boxplot
boxplot(data$T4Cortisol, main = "Boxplot of T4 Cortisol", ylab = "T4 Cortisol")
# Density Plot
plot(density(data$T4Cortisol), main = "Density Plot of T4 Cortisol", xlab = "T4 Cortisol", ylab = "Density")
# Calculate the theoretical quantiles
qqnorm(data$T4Cortisol, main = "QQ Plot of T4Cortisol", xlim = c(min(qqnorm(data$T4Cortisol)$x), max(qqnorm(data$T4Cortisol)$x)), ylim = c(min(qqnorm(data$T4Cortisol)$y), max(qqnorm(data$T4Cortisol)$y) + 2 * IQR(qqnorm(data$T4Cortisol)$y)))
# Add the QQ line
qqline(data$T4Cortisol, col = "red")
```


**Summary statistics**
<img src="image1.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**Histogram**
<img src="image2.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**Density**
<img src="image3.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**Box_Plot**
<img src="image4.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**qq_Plot**
<img src="image5.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**Shapiro-Wilk normality test**
<img src="image15.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

## Categorical Phenotype

I received from Umesh a e-mail informing the three categories that the animals could be sorted based on their cortisol concentration.

<img src="image6.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

```{r eval=FALSE}

data$Categorical <- ifelse(data$T4Cortisol >= 956, "H", 
                           ifelse(data$T4Cortisol <= 190.8, "L", "M"))

table(data$Categorical)
library(ggplot2)

# Reorder the levels of the 'Categorical' column
data$Categorical <- factor(data$Categorical, levels = c("L", "M", "H"))

# Create the histogram with reordered categories
ggplot(data, aes(x = Categorical, fill = Categorical)) +
  geom_bar() +
  labs(title = "Histogram of T4Cortisol by Category",
       x = "Category",
       y = "Frequency") +
  theme_minimal()

# Create the histogram
ggplot(data, aes(x = T4Cortisol, fill = Categorical)) +
  geom_histogram(binwidth = 50, color = "black", alpha = 0.7) + # Adjust binwidth as needed
  labs(title = "Histogram of T4Cortisol with Color by Category",
       x = "T4 Cortisol",
       y = "Frequency",
       fill = "Category") +
  scale_fill_manual(values = c("H" = "red", "M" = "blue", "L" = "green")) + # Adjust colors if needed
  theme_minimal()

# Create the density plot
ggplot(data, aes(x = T4Cortisol, fill = Categorical)) +
  geom_density(alpha = 0.3) +
  labs(title = "Density Plot of T4Cortisol with Color by Category",
       x = "T4Cortisol",
       y = "Density",
       fill = "Category") +
  scale_fill_manual(values = c("H" = "red", "M" = "blue", "L" = "green")) + # Adjust colors if needed
  theme_minimal()

# Create a density plot
ggplot(data, aes(x = T4Cortisol)) +
  geom_density() +
  geom_vline(xintercept = c(956, 190.8), linetype = "dashed", color = "red") +
  labs(title = "Density Plot of T4Cortisol with Vertical Lines",
       x = "T4Cortisol",
       y = "Density") +
  theme_minimal()
```

The animals were sorted in these three categories 
>H = Hight
>M = Medium
>L = Low

<img src="image7.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

The individuals frequency distribution in theese categories are shown in the plots below

<img src="image8.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">
<img src="image9.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">
<img src="image10.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">
<img src="image11.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

## Removing "outliers"

Observing the previous plots I tried to **remove the "outliers"** phenotypes above 1250, but the outcome from Shapiro test is still indicating no normality of the data.

```{r eval=FALSE}
library(tidyverse)

data_no_out <- filter(data, T4Cortisol < 1250)

# Create QQ plot
qqnorm(data_no_out$T4Cortisol, main = "QQ Plot of T4Cortisol", xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
qqline(data$T4Cortisol, col = "red")

boxplot(data_no_out$T4Cortisol, main = "Boxplot of T4 Cortisol", ylab = "T4 Cortisol")

hist(data_no_out$T4Cortisol, breaks = 20, main = "Histogram of T4 Cortisol", xlab = "T4 Cortisol")

shapiro.test(data$T4Cortisol)
```

<img src="image12.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">
<img src="image13.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">
<img src="image14.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">
<img src="image15.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">


# Genetic Correlation

To assess the correlation between Cortisol phenotypes and Genomic Estimated Breeding Values (GEBVs), we opt for a linear regression instead of a standard correlation test. This decision is driven by the non-normal distribution of our Cortisol phenotypes, which violates the assumptions required for traditional correlation tests.

Linear regression offers a robust alternative as it does not necessitate normality for the dependent variable. By regressing GEBVs over Cortisol, we can model the relationship between these variables. Our aim is to estimate the regression coefficient, which serves as our correlation estimate.

Due to the violation of normality assumptions for the dependent variable (Cortisol), traditional correlation tests may not provide reliable results, particularly in assessing the significance of the correlation. Therefore, alternative approaches, such as linear regression, are preferred as they do not require the same assumptions about the distribution of the dependent variable. By using linear regression, we can still assess the relationship between Cortisol and GEBVs while accommodating the non-normality of Cortisol phenotypes.

The regression model can be represented as follows:
\[ y = \beta_0 + \beta_1 \times GEBV_{\text{Milk}} + \epsilon \]

Where:

- \( y \) represents Cortisol phenotypes.
- \( GEBV_{\text{Milk}} \) denotes the GEBV for Milk Yield.
- \( \beta_0 \) and \( \beta_1 \) are the intercept and regression coefficient, respectively.
- \( \epsilon \) represents the error term capturing unexplained variability.

This approach enables us to quantify the relationship between Cortisol and GEBVs, addressing the non-normality of Cortisol phenotypes while allowing for formal hypothesis testing of the correlation's significance.

## Data preparation

The first data I received from Lucas had only 135 animals out of 260 with values the other 125 had only NA
I shown this to Lucas
Lucas wrote to Alisson
Lucas sent me the missing animals
I merged this two files

```{r eval=FALSE}
rm(list = ls())

# Load the necessary library
library(dplyr)
library(tidyverse)

cortisol_260 <- read.csv("/home/bambrozi/2_CORTISOL/Data/data_clean.csv")

#This is the first dataframe with information for 135 animals and 125 NA
GEBVs1 <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/ebvs_elora.csv")
#This is the second file with information for the 125 NA animals
GEBVs2 <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/elora_missing_females_2404_06_11_2024.csv")
#This are de columns we can use because we know the meaning of the acronyms
GEBVs_to_use <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/ebv_names_lucas_06102024_BAG.csv")


sum(is.na(GEBVs1$MILK))
GEBVs1<- GEBVs1[which(is.na(GEBVs1[,"DHI_BARN_NAME"]) == F),]

sum(!is.na(GEBVs2$MILK))
GEBVs2<- GEBVs2[which(is.na(GEBVs2[,"DHI_BARN_NAME"]) == F),]

print(GEBVs1$DHI_BARN_NAME)
print(GEBVs2$DHI_BARN_NAME)

# Making the two dataframes with the same columns
# Remove elora_id and international_id from GEBVs1
GEBVs1 <- GEBVs1 %>% select(-elora_id, -international_id)

# Remove ANIMAL_ID from GEBVs2
GEBVs2 <- GEBVs2 %>% select(-ANIMAL_ID)

# Check if the two dataframes have the same columns
have_same_columns <- all(names(GEBVs1) == names(GEBVs2))

if (have_same_columns) {
  print("The dataframes have the same columns.")
} else {
  print("The dataframes do not have the same columns.")
}


# Check if the column names are in the same order
same_order <- identical(names(GEBVs1), names(GEBVs2))

if (same_order) {
  print("The columns are in the same order.")
} else {
  print("The columns are not in the same order.")
}

GEBVs_combined <- rbind(GEBVs1, GEBVs2)

# Sort the columns
sorted_cortisol_260 <- sort(cortisol_260$ID)
sorted_GEBVs_combined <- sort(GEBVs_combined$DHI_BARN_NAME)

# Check if the sorted columns have the same values
identical(sorted_cortisol_260, sorted_GEBVs_combined)

# Create a duplicate of the column 'DHI_BARN_NAME' and name it 'elora_id'
GEBVs_combined$elora_id <- GEBVs_combined$DHI_BARN_NAME

# Assuming GEBVs_combined is your data frame
GEBVs_combined <- GEBVs_combined %>%
  select(elora_id, DHI_BARN_NAME, everything())

write.csv(GEBVs_combined, "/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/ebvs_elora_complete.csv")

# Merging the dataframe with Cortisol values, with the dataframe with GEBVs values
Merg_Cort_GEBVs <- merge(cortisol_260, GEBVs_combined, by.x = "ID", by.y = "elora_id")

write.csv(Merg_Cort_GEBVs, "/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/Merged_Cortisol_GEBVs.csv")

#Opening the file with the GEBVs columns to use
Columns_to_use <- readLines("/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/traits_to_use.txt")

colnames(Merg_Cort_GEBVs)[405] <- "IDD"

data <- select(Merg_Cort_GEBVs, ID, T4Cortisol, BIRTH_YEAR, all_of(Columns_to_use))

# The data below has the the 55 GEBVs + Cortisol data + Birth Year
write.csv(data, "/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/data_GEBVs_Cortisol_select_traits.csv")

samp_date2 <- read.csv("/home/bambrozi/2_CORTISOL/Data/Elora animal_ids_kl_sampling_date.csv")

# Convert Sampling_date to Date using as.Date
samp_date$Sampling_date <- as.Date(samp_date$Sampling_date, format = "%m/%d/%Y")

table(samp_date$Sampling_date)

samp_date <- select(samp_date, Elora_id, Sampling_date)

# Check if data$ID and samp_dates$elora_id are identical in values and order
identical(data$ID, samp_date$Elora_id)

data_final <- merge(data, samp_date, by.x="ID", by.y="Elora_id")

data_final <- data_final %>%
  select(ID, T4Cortisol, BIRTH_YEAR, Sampling_date, everything())

# The data below has the the 55 GEBVs + Cortisol data + Birth Year + Sampling data
write.csv(data_final, "/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/data_GEBVs_Cortisol_select_traits2.csv")

```

ps. I double checked by hand the select and merge process against the original tables received and is everything ok.

## Correlations - Linear Regression

We tested the minimum model, adding only Birth Year, but adding Birth Year and Sampling Date we got the best results.

### Adding BIRTH_YEAR and SAMPLING DATE

The regression model added the **BY** and **SAMPLING DATE** is shown bellow:

\[ y = \beta_0 + \beta_1 \times GEBV_{\text{Trait}} + BIRTH\_YEAR + SAMPLING\_DATE + \epsilon \]

Where:

- \( y \) represents Cortisol phenotypes.
- \( GEBV_{\text{Trait}} \) denotes the GEBV for the specific trait (e.g., Milk Yield).
- \( BIRTH\_YEAR \) is the birth year of the subjects, included as a **factor**.
- \( SAMPLING\_DATE \) is the cortisol sampling date for the subjects, included as a **factor**.
- \( \beta_0 \) and \( \beta_1 \) are the intercept and regression coefficient, respectively.
- \( \epsilon \) represents the error term capturing unexplained variability.


The `SAMPLING_DATE` variable is also converted to a factor to account for the categorical nature of sampling date.

```{r eval=FALSE}
# Convert BIRTH_YEAR to a factor and rename
data_final$BIRTH_YEAR <- as.factor(data_final$BIRTH_YEAR)

# Convert Sampling_data to a factor and rename
data_final$Sampling_date <- as.factor(data_final$Sampling_date)

# Initialize a list to store the results
results_list <- list()

# Loop through columns 3 to ncol(data) for the GEBVs
for (i in 5:ncol(data_final)) {
  trait_name <- colnames(data_final)[i]
  
  # Fit the linear regression model with BIRTH_YEAR as an additional predictor
  model <- lm(data_final[[2]] ~ data_final[[i]] + data_final$BIRTH_YEAR + data_final$Sampling_date , data = data_final)
  
  # Summarize the model
  model_summary <- summary(model)
  
  # Extract the desired statistics
  multiple_r_squared <- model_summary$r.squared
  adjusted_r_squared <- model_summary$adj.r.squared
  f_statistic <- model_summary$fstatistic[1] # F-statistic value
  f_num_df <- model_summary$fstatistic[2] # Numerator degrees of freedom
  f_den_df <- model_summary$fstatistic[3] # Denominator degrees of freedom
  p_value <- pf(f_statistic, f_num_df, f_den_df, lower.tail = FALSE) # P-value
  
  # Extract the coefficient and its p-value for the trait
  coef_summary <- coef(model_summary)
  trait_coef <- coef_summary[2, "Estimate"]  # Assumes the trait is the second predictor
  trait_p_value <- coef_summary[2, "Pr(>|t|)"]
  
  # Combine the statistics into a data frame
  result <- data.frame(
    Trait = trait_name,
    Multiple_R_Squared = multiple_r_squared,
    Adjusted_R_Squared = adjusted_r_squared,
    F_Statistic = f_statistic,
    P_Value = p_value,
    Coefficient = trait_coef,
    Coefficient_P_Value = trait_p_value
  )
  
  # Append the result to the results list
  results_list[[i - 2]] <- result
}

# Combine all results into a single data frame
results_df <- do.call(rbind, results_list)

# Save the results to a CSV file
write.csv(results_df, file = "/home/bambrozi/2_CORTISOL/Correlation/Results/add_BY_SAMP/regression_summary_all_traits_BY_SampDt.csv", row.names = FALSE)
```

<span style="color: blue;">**Summary statistics for all Traits' GEBVs adding BIRTY_YEAR and SAMPLING_DATE**</span>
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/regression_summary_all_traits_BY_SampDt.csv")

# Print the table using knitr::kable
knitr::kable(data)

```


Fitting Birth_Year and Sampling_date to the model these are the traits with significant correlation **(<0.15)**:
<ul>
<li> CO = Cystic ovaries
<li> BMR = Body Maintenance Requirements
<li> LP = Lactation persistency
<li> MILK = Milk yield
<li> PROT = Protein yield
<li> UT = Udder Texture
<li> CK = Clinical Ketosis
<li> HHE = Heel Horn Erosion
<li> MSP = Milking Speed
</ul>


# GENOTYPES

Lucas Alcântara sent me the path to the genotype and pedigree files:
/data/cgil/daiclu/6_Data/database/public_output/bruno

<img src="image17.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

In this folder we found the following files:

<ul>
<li>**bruno_gntps.txt**: the file with the genotypes in  **012 format**
<li>**bruno_ids.csv**: file with elora_id, cdn_id, iid
<li>**bruno_ped.id.csv**: pedigree with cdn_id
<li>**bruno_ped.iid.csv**: pedigree with iid
</ul>

I made a copy of this files in a folder called **Raw_files**:

<span style="color: blue;">/home/bambrozi/2_CORTISOL/RawFiles</span>

This directory has two sub-directories:
<ul>
<li> Genotypes
<li> Pedigree
</ul>


## Transforming the Genotypes in 012 format to plink format

To perfome this I used two codes

1) Add "column names"
```{bash eval=F}
sed -i '1i id Call' genotypes.txt
```


2) Recode in python
```{python eval=FALSE, python.reticulate = FALSE}
filename = 'bruno_gntps.txt'

outputFileOpen = open('genoplink.ped','w')


recode = {'0':['C','C'] , '1':['A','C'] , '2':['A','A'] , '5':['0','0'] }
for line in open(filename,'r'):
    if 'Call' in line : continue


    ids, call = line.strip().split()
    genotypes = [ recode[geno012][0] +' '+ recode[geno012][1] for geno012 in call ]


    outputFileOpen.write("%s %s %s %s %s %s %s\n" % ('HO', ids, '0','0','0','-9',' '.join(genotypes)) )

```

Now I have a folder called **/home/bambrozi/2_CORTISOL/Geno_files** with the file named **genoplink.ped**


# PHENOTYPE file

```{r eval=FALSE}
library(data.table)

pheno <- read.csv("/home/bambrozi/2_CORTISOL/Data/data_clean.csv")
ped <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/Pedigree/bruno_ids.csv")
geno <- fread("/home/bambrozi/2_CORTISOL/Geno_files/genoplink.ped")
geno <- geno[,c("V2")]

#Bringing cdn_id to my phenotype file
#Generate a index with the match
matching_indices <- match(pheno$ID, ped$elora_id)
# Then, assign 'cdn_id' from 'ped' to 'pheno' where there are matches
pheno$cdn_id <- ifelse(!is.na(matching_indices), ped$cdn_id[matching_indices], NA)

#Making a phenotype file only with genotyped animals
pheno_genotyped <- pheno[pheno$cdn_id %in% geno$V2,] 

#check if all animals in this file are genotyped
checkk <- pheno_genotyped$cdn_id %in% geno$V2
sum(checkk)

#The phenotype file should have three columns: FID, Animal_id, Phenotype
HO <- rep("HO", 252)

pheno_gwas <- as.data.frame(cbind(HO, pheno_genotyped$cdn_id, pheno_genotyped$T4Cortisol))

colnames(pheno_gwas) <- c("FID", "cdn_id", "cortisol")

pheno_gwas$cdn_id <-  as.numeric(pheno_gwas$cdn_id)
pheno_gwas$cortisol <- round(as.numeric(pheno_gwas$cortisol),2)

write.table(pheno_gwas, "/home/bambrozi/2_CORTISOL/GWAS/pheno_genotyped.txt", quote = F, row.names = F, col.names = T)
```


# SNP MAP

Adjusting the SNP_map to .map

```{r eval=FALSE}
map <- fread("/data/cgil/daiclu/6_Data/database/public_output/bruno/DGVsnpinfo.2404.ho")
morgan <- data.frame(X0 = rep(0, 45101))
mapa=as.data.frame(cbind(map$chr, map$snp_name, morgan$X0, map$location))
head(mapa)
write.table(x = mapa, file = "/home/bambrozi/2_CORTISOL/Geno_files/genoplink.map", row.names = FALSE, col.names = FALSE, sep = "\t", quote = FALSE)
```

## Generating the bfiles

```{r eval=FALSE}
system("/home/local/bin/plink --cow --nonfounders --allow-no-sex --keep-allele-order --file /home/bambrozi/2_CORTISOL/Geno_files/genoplink --make-bed --out /home/bambrozi/2_CORTISOL/Geno_files/genoplink")
```

With the code above I generated the bfiles:
<ul>genoplink.bed
<li>genoplink.bim
<li>genoplink.fam
<li>genoplink.log
<li>genoplink.nosex
</ul>


# Quality Control

We ran the code below to perfom the QC
okok
```{bash eval=F}
#!/bin/bash

DATA=/home/bambrozi/2_CORTISOL/Geno_files/genoplink
RESULT=/home/bambrozi/2_CORTISOL/Geno_files_after_QC/genoplink_clean

/home/local/bin/plink \
    --bfile ${DATA} \
    --cow \
    --allow-no-sex \
    --hwe 1e-5 \
    --maf 0.01 \
    --geno 0.1 \
    --mind 0.1 \
    --keep-allele-order \
    --make-bed \
    --out ${RESULT}
    
```


The server screen outcome is shown below.
<img src="image18.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

After the Quality Control we ended up with

<ul>
<li> 252 samples
<li> 42,278 variants (SNPs)
</ul>

# KING

To check for duplicated individuals I performed the KINSHIP analysis using one script from Larissa Braga. Running the King Analysis on Plink.
okok
```{bash eval = FALSE}
#!/bin/bash

DATA=/home/bambrozi/2_CORTISOL/Geno_files_after_QC/genoplink_clean
RESULT=/home/bambrozi/2_CORTISOL/Geno_files_after_KING/result_king

plink2 \
    --bfile ${DATA} \
    --cow \
    --make-king-table \
    --out ${RESULT}
```

This is the output screen on terminal:

<img src="image19.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

The table below is the output <span style="color: blue;">/home/bambrozi/2_CORTISOL/Geno_files_after_KING/result_king.kin0</span> and have pairwise comparisons between all individuals. 

<img src="image20.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

Now we should open in R and check for individuals with more than 0,354, to perform this we can use the code below, also provided by Larissa Braga:
okok
```{r eval=FALSE}
setwd("/home/bambrozi/2_CORTISOL/Geno_files_after_KING")

relatedness="result_king.kin0" ## change accordingly!!

library(data.table)

print(relatedness)
rel=fread(relatedness, h = T)
head(rel)

print("Individuals with different identifications above the cut off of 0.354:")
dup=subset(rel, KINSHIP >= 0.354  & IID1!=IID2)
print(dup)
nrow(dup)

```

So the code above will provide this output if there is no duplicated individual.

<img src="image21.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**We do not have any duplicated individual**

So the file to be used are those in the directory <span style="color: blue;">/home/bambrozi/2_CORTISOL/Geno_files_after_QC</span>

>files:genoplink_clean

After Quality Control we didn't lost any animal, so we don't need to update our phenotype file

# PCA

Now before performing the PCA analysis we need to change the FID for those individuals that has phenotype = **1** for Nadia.


```{bash eval=F}
#!/bin/bash

DATA=/home/bambrozi/2_CORTISOL/Geno_files_after_QC/genoplink_clean
RESULT=/home/bambrozi/2_CORTISOL/PCA/pca_result

plink \
    --bfile ${DATA} \
    --keep-allele-order \
    --cow \
    --pca \
    --out ${RESULT}

```

The PCA output:

<img src="image22.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

## PCA Plot

After generate the Eigenvalues and Eigenvectors I used the code below to generate the PCA Plot

```{r eval=FALSE}
setwd("/home/bambrozi/2_CORTISOL/PCA")

library(ggplot2) 
library(tidyverse)

##
# Visualize PCA results
###

# read in result files
eigenValues <- read_delim("pca_result.eigenval", delim = " ", col_names = F)
eigenVectors <- read_delim("pca_result.eigenvec", delim = " ", col_names = F)

## Proportion of variation captured by each vector
eigen_percent <- round((eigenValues / (sum(eigenValues))*100), 2)


# PCA plot
png("pca-plink.eng.png", width=5, height=5, units="in", res=300)
ggplot(data = eigenVectors) +
  geom_point(mapping = aes(x = X3, y = X4), color = "red", shape = 19, size = 1, alpha = 1) +
  geom_hline(yintercept = 0, linetype="dotted") +
  geom_vline(xintercept = 0, linetype="dotted") +
  labs(x = paste0("Principal component 1 (", eigen_percent[1,1], " %)"),
       y = paste0("Principal component 2 (", eigen_percent[2,1], " %)")) +
  theme_minimal()
dev.off()


# PCA plot with animal ids
png("pca-plink.eng.animals_id.png", width=50, height=50, units="in", res=300)
ggplot(data = eigenVectors) +
  geom_point(mapping = aes(x = X3, y = X4), color = "red", shape = 19, size = 5, alpha = 1) +
  geom_text(mapping = aes(x = X3, y = X4, label = X2), size = 2, hjust = 0, vjust = 0) +  # Add labels for animal IDs
  geom_hline(yintercept = 0, linetype="dotted") +
  geom_vline(xintercept = 0, linetype="dotted") +
  labs(x = paste0("Principal component 1 (", eigen_percent[1,1], " %)"),
       y = paste0("Principal component 2 (", eigen_percent[2,1], " %)")) +
  theme_minimal()
dev.off()
```

<img src="image23.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

# GWAS on GCTA

Previously we have performed GWAS on GCTA:
<ul> 
<li> With the whole sample and no fixed effects
<li> With the whole sample and with Birht Year as fixed effect
<li> With the whole sample and with Birth Year and Sampling Data as fixed effects
</ul>


## GWAS - EXTREME PHENO - WITH BY and SD

### Data preparation

I received from Umesh a e-mail informing the three categories that the animals could be sorted based on their cortisol concentration.

<img src="image6.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

```{r eval=FALSE}

pheno <- read.table("/home/bambrozi/2_CORTISOL/GWAS/pheno_genotyped.txt", header = T)
data_final <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/data_GEBVs_Cortisol_select_traits2.csv", header = T)
ids_eq <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/Pedigree/bruno_ids.csv", header = T)


# Create an matrix with fixed effects with only those animals which also have phenotype and genotype
data_final$cdn_id <- ids_eq$cdn_id[match(data_final$ID, ids_eq$elora_id)]
fixeff <- data_final[,c("ID", "BIRTH_YEAR", "Sampling_date", "cdn_id")]
fixeff <- fixeff[fixeff$cdn_id %in% pheno$cdn_id, ]
fixeff$FID <- "HO"
fixeff <- fixeff[, c("FID", "cdn_id", "BIRTH_YEAR", "Sampling_date")]

# Now we are gona remove the intermediary animals from pheno object
pheno$Categorical <- ifelse(pheno$cortisol >= 956, "H", 
                           ifelse(pheno$cortisol <= 190.8, "L", "M"))
table(pheno$Categorical)
pheno <- pheno[pheno$Categorical != "M", ]
pheno <- pheno[, c("FID", "cdn_id", "cortisol")]

# Now we are going to remove from fixeff the animals which are not in pheno
fixeff <- fixeff[fixeff$cdn_id %in% pheno$cdn_id,]

#Checking if match the animals and order
identical(fixeff$cdn_id, pheno$cdn_id)

#Creating a file with animals to keep in the genotype file, we will use it on Plink
to_keep_geno <- pheno[, c("FID", "cdn_id")]

write.table(fixeff, "/home/bambrozi/2_CORTISOL/GWAS/EXTREM_PHENO_BY_SD/fixeff.txt", quote = F, row.names = F, col.names = T)
write.table(pheno, "/home/bambrozi/2_CORTISOL/GWAS/EXTREM_PHENO_BY_SD/pheno.txt", quote = F, row.names = F, col.names = T)
write.table(to_keep_geno, "/home/bambrozi/2_CORTISOL/GWAS/EXTREM_PHENO_BY_SD/to_keep_geno.txt", quote = F, row.names = F, col.names = F)

```

We ended up with  
H (Hight) = 34 animals  
L (Low)  = 37 animals
Total = 71 animals

On Plink we will remove all individuals from genotype files that are classified as Medium, keeping only the High and Low

```{bash eval= FALSE}
#!/bin/bash

DATA=/home/bambrozi/2_CORTISOL/Geno_files_after_QC/genoplink_clean
RESULT=/home/bambrozi/2_CORTISOL/GWAS/EXTREM_PHENO_BY_SD/geno_extreme
KEEP=/home/bambrozi/2_CORTISOL/GWAS/EXTREM_PHENO_BY_SD/to_keep_geno.txt

plink2 --bfile ${DATA} --keep ${KEEP} --chr-set 29 --make-bed --out ${RESULT}
```

### GWAS on GCTA

```{bash eval=FALSE}
#!/bin/bash

DATA=/home/bambrozi/2_CORTISOL/GWAS/EXTREM_PHENO_BY_SD/geno_extreme
RESULT=/home/bambrozi/2_CORTISOL/GWAS/EXTREM_PHENO_BY_SD/GWAS_result
PHENO=/home/bambrozi/2_CORTISOL/GWAS/EXTREM_PHENO_BY_SD/pheno.txt
FIXEFF=/home/bambrozi/2_CORTISOL/GWAS/EXTREM_PHENO_BY_SD/fixeff.txt

/home/local/bin/gcta \
    --bfile ${DATA} \
    --mlma-loco \
    --pheno ${PHENO} \
    --qcovar ${FIXEFF} \
    --autosome-num 29 \
    --autosome \
    --out ${RESULT}
```

After ran the GWAS above I got the following message from the GCTA:

<span style="color:red">**Error: analysis stopped because more than half of the variance components are constrained. The result would be unreliable. You may have a try of the option --reml-no-constrain.**</span>

As we got this error message, we needed to solve this problem, and for that we used the whole sample size (252 individuals) to estimate the variance components, and after this, using this variance components from the whole sample we performed the ssGWAS with the subset of individuals (34 High + 37 Low = 71), but this was not possible in GCTA so we switched to another software (**BLUPF90+**)

# BLUPF90+ GWAS

To run ssGWAS on Blupf90+ suite, we will need 4 different softwares:

<ul>
<li> **renum**: just to renumerate the files and generate the renf90.par
<li> **preGSF90**: just to perfome a quality control with different parameter from the default.
<li> **blupf90+**: used to estimate VCE and generate Ainv and Ginv
<li> **postGSF90**: perform GWAS
</ul>

The tutorial for preGSF90 and postGSF90 we can find in the link bellow
<https://nce.ads.uga.edu/wiki/doku.php?id=readme.pregsf90#gwas_options_postgsf90>

According to the BLUPF90+ tutorial:

    ssGWAS is an iterative procedure with 2 steps:
    0) Quality control
    1) prediction of GEBV with ssGBLUP
    2) prediction of SNP marker effects based on the GEBV

## Files preparation

Preparing files to run Variance components estimation using REML with AI (Average Information) algorithm.

First you need to create a directory in your home directory, prepare and save the following files in:

<ul>
<li> Phenotype and Fixed effects file
<li> Pedigree file
<li> Genotype file
<li> BlupF90+ executable file
<li> RenumF90 executable file
<li> preGSf90 executable file
<li> postGSf90 executable file
<li> Parameter file
<ul>



### Phenotype and Fixed effects file

The appearance of this file is like this:

<img src="image95.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**FIRST COLUMN** = Animal ID  
**SECOND COLUMN** = Phenotype  
**THIRD COLUMN** = Fixed Effect 1  
**FOURTH COLUMN** = Fixed Effect 2  

First we are going to generate a Phenotype_Fixed_Effect file with the whole sample (252 individuals) that we are going to use for the Variance Components Estimation.

To get in one file these four columns we need the following code:


```{r eval=FALSE}
pheno <- read.table("/home/bambrozi/2_CORTISOL/GWAS/pheno_genotyped.txt", header = T)
data_final <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/data_GEBVs_Cortisol_select_traits2.csv", header = T)
ids_eq <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/Pedigree/bruno_ids.csv", header = T)


# Create data_final$cdn_id by matching IDs with elora_id
data_final$cdn_id <- ids_eq$cdn_id[match(data_final$ID, ids_eq$elora_id)]
fixeff <- data_final[,c("ID", "BIRTH_YEAR", "Sampling_date", "cdn_id")]
fixeff <- fixeff[fixeff$cdn_id %in% pheno$cdn_id, ]
fixeff$FID <- "HO"
fixeff <- fixeff[, c("FID", "cdn_id", "BIRTH_YEAR", "Sampling_date")]

identical(fixeff$cdn_id, pheno$cdn_id)

fixeff <- fixeff[, c("cdn_id", "BIRTH_YEAR", "Sampling_date")]
pheno <- pheno[,c("cdn_id", "cortisol")]

# Load necessary libraries
library(dplyr)

# Merge pheno and fixeff data frames
merged_data <- pheno %>% 
  left_join(fixeff, by = "cdn_id")


merged_data$iid <- ids_eq$iid[match(merged_data$cdn_id, ids_eq$cdn_id)]

merged_data <- merged_data[, c("iid", "cortisol", "BIRTH_YEAR", "Sampling_date")]

write.table(merged_data, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/fenofix.txt", col.names = F, quote = F, row.names = F)


```


The file should be saved as text file, with separation by space and no columns names.

PS: if there are any **NA**, they sould be replaced by **9999**

/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/fenofix.txt **(this file has 252 individuals)**


### Pedigree file

The appearance of this file is like this:

<img src="image96.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">  

**FIRST COLUMN** = Animal ID  
**SECOND COLUMN** = Sire ID  
**THIRD COLUMN** = Dam ID  

The file should be saved as text file, with separation by space and no columns names.

We used the code below to remove the commas of a .csv file to a file with sepation by spaces.
```{bash eval=FALSE}
# to replace comma for space in the .csv file with the equivalence among IDs
sed -i 's/,/ /g' bruno_ids.csv
```

### Genotype file

First we are going to generate a Genotype file with the whole sample (252 individuals) that we are going to use for the Variance Components Estimation.

The appearance of this file is like this:

<img src="image97.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**FIRST COLUMN** = Animal ID
**SECOND COLUMN** = Genotypes (0, 1 and 2 format)

The file should be saved as text file, with separation by space and no columns names.

We  used the code below to replace the cid for iid. First we merge using the second column of the firs file, and the first column of the second file. Then we use again the command awk to keep only the third and fifth columsn and sabe in a different object.



```{bash eval=FALSE}
# Using the awk function to merge the two files and the second awk to select only the 3rd and 5fh columns
awk 'FNR==NR {a[$2]=$0; next} {print a[$1], $0}' bruno_ids.csv bruno_gntps.txt | awk '{print$3,$5}' > bruno_gntps_iid
```

Below we can find the file's location from the code above:
/home/bambrozi/2_CORTISOL/RawFiles/Genotypes/bruno_gntps.txt
/home/bambrozi/2_CORTISOL/RawFiles/Pedigree/bruno_ids.csv
/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/bruno_gntps_iid **(this file has 252 individuals)**


### Download the executable files

Download from this website <https://nce.ads.uga.edu/html/projects/programs/Linux/64bit/>:
<ul>
<li> BlupF90+ = we will use to estimate the Variance components and GEBVs
<li> renumF90 = we will use to renumerate the files
<li> preGSf90 = we will use to perform the Quality control
<li> postGSf90 = we will use for GWAS
<ul>

### SNP MAP

```{r eval = FALSE}
mapfile <- read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/DGVsnpinfo.2404.ho", header = T)

colnames(mapfile)

colnames(mapfile) <- c("SNP_ID", "CHR", "POS")

mapfile <- mapfile[,c("CHR", "POS", "SNP_ID")]

write.table(mapfile, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/snpmap.txt", col.names = T, row.names = F, quote = F)
```


## Variance component estimation

But, before running the GEBV we will first perform one additional step to "CLEAN" our genotypes. Actually BLUPF90 by default perform a data cleaning with pre set parameters, but we will change some default parameters an so perform this additional step.

The Parameter card for this step is the parameter bellow:

/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/renum_QC.par

```{bash eval= FALSE}

DATAFILE
fenofix.txt
TRAITS
2
FIELDS_PASSED TO OUTPUT

WEIGHT(S)

RESIDUAL_VARIANCE
1.0
EFFECT
3 cross numer
EFFECT
4 cross alpha
EFFECT
1 cross alpha
RANDOM
animal
FILE
bruno_ped_iid_blup.txt
FILE_POS
1 2 3 0 0
SNP_FILE
bruno_gntps_iid
PED_DEPTH
0
(CO)VARIANCES
1.0
OPTION outcallrate
OPTION saveCleanSNPs
OPTION minfreq 0.01
OPTION map_file snpmap.txt
OPTION excludeCHR 30 31
```

<ul>
<li> **DATAFILE**: bellow this line you need to inform the name of the file with phenotype and fixed effects. As before running BLUPF90 on server you are going to direct the terminal to the directory where all these files are placed you only need to inform the name.
<li> **TRAITS**: below this line you need to inform which column are the phenotype date in the previous file, in this example, 2.
<li> **FIELDS_PASSED TO OUTPUT**:
<li> **WEIGHT (S)**:
<li> **RESIDUAL VARIANCE**: for the firs run you need to inform the value of 1.0, for the second you can pick the variance from the firs run's output.
<li> **EFFECT**: you will inform your first effect, in this example, Birth Year, which is in the column 3, and the word **cross numer** because is a number.
<li> **EFFECT**: you should provide the next effect, in this example, sample date, as sample date has one non numeric character you should inform as **cross alpha**, in this example column 4.
<li> **EFFECT**: now I'm providing my animal ID information, in this example column 1, and again **cross alpha** because has number and letters in the ID. I'm also informing that this effect is **RANDOM**, and that is my **animal** effect.
<li> **FILE**: bellow this line I need to provide the pedigree file. Again, as I'm already in the directory which contain the pedigree file I only need to provide the file name.
<li> **FILE-POS**: Here I'll inform which columns should be considered in the pedigree file, in this situation, **1 2 3 0 0**.
<li> **PED_DEPTH**: Now we can inform the depth we want the software considers the pedigree, or if we leave **0** it will the maximum possible.
<li> **(CO) VARIANCES**: Here you should provide the Variance/Co-variance matrix, like as for residual variance in the first run we set up to 0 in this example that we don´t have to imagine any co-variance, but if you know that exist variance among you effects you shoul set up **XXX** for ....
<li> **OPTION outcallrate**: Save the call rate information on SNP markers in the file.
<li> **OPTION saveCleanSNPs**: This option generates 4 new files. We assume snpfile as a marker file.
<ul>
<li> snpfile_clean = new SNP marker file.
<li> snpfile_clean_XrefID = new cross-reference file.
<li> snpfile_SNPs_removed = a list of removed markers.
<li> snpfile_Animals_removed = a list of removed animals.
</ul>
<li> **OPTION minfreq 0.01**: Minimum allele frequency to retain the marker.
<li> **OPTION map_file snpmap.txt**: This option will upload the SNP MAP
<li> **OPTION excludeCHR 30 31**: This option will remove **sexual chromosome** that is the 30 and 31
</ul>

To run any softwere from Blupf90 suit we will perform always in this way:

1. Go to the server you wanna run this analysis, for instance, **grand**
```{bash eval=F}
ssh grand
```

2. Now go to the directory you have created to run this analysis where that set of files are placed.
```{bash eval = F}
cd /home/bambrozi/2_CORTISOL/GWAS/BLUPF90
```

3. Make the renumF90 and BlupF90+ files executables
```{bash eval=F}
chmod +x renumf90
chmod +x blupf90+
```

4. Run renumF90
```{bash eval=F}
./renumf90
```

When you run the code above, it will ask you the name of your parameter card, for this step is **renum_QC.par**.

The command above will generate couple files, among them **renf90.par**

We modified **renf90.par** in:
<ul>
<li> renf90_DataClean.par
<li> renf90_VarCompEst.par
</ul>

### Quality control

The parameter card to perform the **Quality Control** is: 
/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/renf90_DataClean.par

It will run using the software pre **preGS90** to generate the Clean Genotype and SNP_MAP files after Quality Control.

```{bash eval= FALSE}
# BLUPF90 parameter file created by RENUMF90
DATAFILE
 renf90.dat
NUMBER_OF_TRAITS
           1
NUMBER_OF_EFFECTS
           3
OBSERVATION(S)
    1
WEIGHT(S)
 
EFFECTS: POSITIONS_IN_DATAFILE NUMBER_OF_LEVELS TYPE_OF_EFFECT[EFFECT NESTED]
 2         4 cross 
 3        23 cross 
 4      3724 cross 
RANDOM_RESIDUAL VALUES
   1.0000    
 RANDOM_GROUP
     3
 RANDOM_TYPE
 add_an_upginb
 FILE
renadd03.ped                                                                    
(CO)VARIANCES
   1.0000    
OPTION SNP_file bruno_gntps_iid
OPTION outcallrate
OPTION saveCleanSNPs
OPTION minfreq 0.01
OPTION map_file snpmap.txt
OPTION excludeCHR 30 31

```


### VCE

The second parameter card used for Variance Components Estimation (VCE) is the following:
/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/renf90_VarCompEst.par

It will be run using the software pre **blupf90+** to generate the VCE.

```{bash eval= FALSE}
# BLUPF90 parameter file created by RENUMF90
DATAFILE
 renf90.dat
NUMBER_OF_TRAITS
           1
NUMBER_OF_EFFECTS
           3
OBSERVATION(S)
    1
WEIGHT(S)
 
EFFECTS: POSITIONS_IN_DATAFILE NUMBER_OF_LEVELS TYPE_OF_EFFECT[EFFECT NESTED]
 2         4 cross 
 3        23 cross 
 4      3724 cross 
RANDOM_RESIDUAL VALUES
   1.0000    
 RANDOM_GROUP
     3
 RANDOM_TYPE
 add_an_upginb
 FILE
renadd03.ped                                                                    
(CO)VARIANCES
   1.0000    
OPTION SNP_file bruno_gntps_iid_clean
OPTION no_quality_control
OPTION method VCE
OPTION origID
OPTION missing 9999
OPTION se_covar_function H2_1 g_3_3_1_1/(g_3_3_1_1+r_1_1)


```

<ul>
<li> **OPTION SNP_file bruno_gntps_iid_clean**: we are going to inform the genotype file generated in the previous step (the Quality Control). Blup will create an file with the same name that the original genotype file, and add the sufix **_clean**
<li>**OPTION no_quality_control** we need to set up this option because we performed Quality Control in the previous step and now we don't need that Blupf90+ perform again. Blupf90+ by default perform quality control, so if we do not want, we need to specify.
<li> **OPTION method**: VCE (Variance Component Estimation).
<li> **OPTION OrigID**: this will keep the original ID informed.
<li> **OPTION missing 9999**: you are informing that missing values will appear as **9999**
<li> **OPTION se_covar_function**: H2_1 g_3_3_1_1/(g_3_3_1_1+r_1_1)
<ul> 
<li> **H2_1**: the name that your function will appear on the output files.
<li> **g_3_3**: you are asking for genetic variance estimation for the 3rd informed effect.
<li> **_1_1**: this effect is in the 1st column. 
<li> **/(g_3_3_1_1+r_1_1)**: to get the total phenotipic variance, you are summing to genetic variance the residual variance of the effect in column 1.
</ul>
</ul>

In the parameter card above, we remove the option for Quality Control and added options for Variance Components Estimation, for Missing data, for origID and for heritability estimation, but the MOST IMPORTANT PART is we need to change the OPTION SNP_FILE, replacing the original genotype file, for the "clean" version generated in the previous step.

The Variance Components will be placed in the file: 
/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/blupf90.log

**blupf90.log**  
<img src="image128.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;"> 


Now you should update you **renf90_VarCompEst.par** file with these informations from the .log file

Copy **Residual Variance** from blupf90.log and will paste on renf90_VarCompEst.par **RANDOM_RESIDUAL_VALUES**
Copy **Genetic variance for effect x** from blupf90.log and will paste on renf90_VarCompEst.par **(CO) VARIANCE**


If the **Residual Variance** and **Genetic variance for effect x** didn't change in your blupf90.log the analysis ended, but if this value vary, you should update again the renf90.par and run again blupf90+ until this values don't change more.


**blupf90.log**  
<img src="image128.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">  

**Parameter card**  
<img src="image127.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">  



Now that we have the Variance components we go for the next step:
<ul>
<li> Prediction of SNP marker effects based on the GEBV
<li> GWAS for High and Low cortisol animals
</ul>

To do this, I'll: 
<ul>
<li> generate the Phenotype_FixEff file like bellow
<li> generate the genotype file like bellow
<li> Perform the QUALITY CONTROL for the new (71 samples) genotype file
<li> Add the VCE from the 252 data set in the parameter card
<li> Generate the GEBV
<li> Run GWAS
</ul>

### Prediction of SNP marker effects based on the GEBV

#### GWAS for High and Low cortisol animals

Now we'll run a new analysis using the Variance Components Estimation from the previous step to perform the GWAS.

To perform this first we need a a Genotype file only with the 71 animals (High=34 and Low=37)

##### Updating the files

###### Phenotype and Fixed Effects files

In this read_me file we performed a ssGWAS but keeping the phenotype and fixed effect information for all 252 individuals, only reducing the number of individuals for the Genotype file.

###### Genotype files
I used this command line bellow to remove the individuals with MEDIUM phenotypes.
```{bash eval= FALSE}
awk 'NR==FNR{ids[$1]; next} $1 in ids' fenofix.txt bruno_gntps_iid > bruno_gntps_iid_71
```

#### Running renum_QC.par

/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/renum_QC.par

```{bash eval = FALSE}
DATAFILE
fenofix.txt
TRAITS
2
FIELDS_PASSED TO OUTPUT

WEIGHT(S)

RESIDUAL_VARIANCE
77182
EFFECT
3 cross numer
EFFECT
4 cross alpha
EFFECT
1 cross alpha
RANDOM
animal
FILE
bruno_ped_iid_blup.txt
FILE_POS
1 2 3 0 0
SNP_FILE
bruno_gntps_iid_71
PED_DEPTH
0
(CO)VARIANCES
28212
OPTION outcallrate
OPTION saveCleanSNPs
OPTION minfreq 0.01
OPTION map_file snpmap.txt
OPTION excludeCHR 30 31
```


We modified **renf90.par** in 3 copies:
<ul>
<li> renf90_DataClean.par
<li> renf90_ssGWAS1_Ginv.par
<li> renf90_ssGWAS2_SNPeff.par
</ul>


#### Running renf90_DataClean.par for Quality Control

To run the parameter card bellow we are going to use the software **presGSf90**
/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/renf90_DataClean.par

```{bash eval = FALSE}
# BLUPF90 parameter file created by RENUMF90
DATAFILE
 renf90.dat
NUMBER_OF_TRAITS
           1
NUMBER_OF_EFFECTS
           3
OBSERVATION(S)
    1
WEIGHT(S)
 
EFFECTS: POSITIONS_IN_DATAFILE NUMBER_OF_LEVELS TYPE_OF_EFFECT[EFFECT NESTED]
 2         4 cross 
 3        22 cross 
 4      3724 cross 
RANDOM_RESIDUAL VALUES
   77182.    
 RANDOM_GROUP
     3
 RANDOM_TYPE
 add_an_upginb
 FILE
renadd03.ped                                                                    
(CO)VARIANCES
   28212.    
OPTION SNP_file bruno_gntps_iid_71
OPTION outcallrate
OPTION saveCleanSNPs
OPTION minfreq 0.01
OPTION map_file snpmap.txt
OPTION excludeCHR 30 31
```

#### Running renf90_ssGWAS1_Ginv.par for Ginv estimation

May be necessary to run the command bellow on the server
Setting the stack size to "unlimited" allows the program to allocate memory for these large structures without hitting stack limits. By removing stack size limits, BLUPF90 is less likely to encounter **segmentation faults** or memory allocation issues that arise when the stack space is insufficient for the computations needed.
```{bash eval = FALSE}
ulimit -s unlimited
```

The parameter card bellow we are going to run using the software **blupf90+**:

/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/renf90_ssGWAS1_Ginv.par

```{bash eval=FALSE}
# BLUPF90 parameter file created by RENUMF90
DATAFILE
 renf90.dat
NUMBER_OF_TRAITS
           1
NUMBER_OF_EFFECTS
           3
OBSERVATION(S)
    1
WEIGHT(S)
 
EFFECTS: POSITIONS_IN_DATAFILE NUMBER_OF_LEVELS TYPE_OF_EFFECT[EFFECT NESTED]
 2         4 cross 
 3        22 cross 
 4      3724 cross 
RANDOM_RESIDUAL VALUES
   77182.    
 RANDOM_GROUP
     3
 RANDOM_TYPE
 add_an_upginb
 FILE
renadd03.ped                                                                    
(CO)VARIANCES
   28212.    
OPTION SNP_file bruno_gntps_iid_71_clean
OPTION no_quality_control
OPTION origID
OPTION missing 9999
OPTION saveGInverse
OPTION saveA22Inverse
OPTION snp_p_value
```


#### Running renf90_ssGWAS2_SNPeff.par for GWAS

The parameter card bellow we are going to run using the software **postGSf90**:
/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/renf90_ssGWAS2_SNPeff.par

```{bash eval= FALSE}
# BLUPF90 parameter file created by RENUMF90
DATAFILE
 renf90.dat
NUMBER_OF_TRAITS
           1
NUMBER_OF_EFFECTS
           3
OBSERVATION(S)
    1
WEIGHT(S)
 
EFFECTS: POSITIONS_IN_DATAFILE NUMBER_OF_LEVELS TYPE_OF_EFFECT[EFFECT NESTED]
 2         4 cross 
 3        22 cross 
 4      3724 cross 
RANDOM_RESIDUAL VALUES
   77182.    
 RANDOM_GROUP
     3
 RANDOM_TYPE
 add_an_upginb
 FILE
renadd03.ped                                                                    
(CO)VARIANCES
   28212.    
OPTION SNP_file bruno_gntps_iid_71_clean
OPTION origID
OPTION no_quality_control
OPTION readGInverse
OPTION readA22Inverse
OPTION map_file snpmap.txt_clean
OPTION snp_p_value
OPTION Manhattan_plot_R
OPTION Manhattan_plot 
```

This code will generate several files, among them:
<ul>
<li> **chrsnp_pval**: 
<ul>
<li> Column 1: trait
<li> Column 2: effect
<li> Column 3: -log10(p-value)
<li> Column 4: SNP
<li> Column 5: Chromosome
<li> Column 6: Position in bp
</ul>
<li> **Pft1e3.R**: a r-code to generate the Manhattan plot in R using the chrsnp_pval
</ul>


#### Manhattan Plots for BLUPF90 GWAS

##### Genome Independent Segment
To make the Manhattan Plot considering Genome Independent Segment we should run the code bellow. This code has part of the code in the file **Pft1e3.R**

```{r eval=FALSE}
Genome_Assembly_ARS_UCD_1_2 <- read_tsv("/home/bambrozi/2_CORTISOL/GWAS/sequence_report_ARS-UCD1_2.tsv")

library(dplyr)
# Filter the rows and sum the Seq length column
# Assuming your data frame is named Genome_Assembly_ARS_UCD_1_2
L <- Genome_Assembly_ARS_UCD_1_2 %>%
  filter(`UCSC style name` %in% paste0("chr", 1:29)) %>%
  summarise(total_length = sum(`Seq length`)) %>%
  pull(total_length)

# Converting bases to Morgan (1Mb = 1cM (0,01 Morgan))
L_M <- L/10^8

# The Ne measure is based on the article bellow:
Ne <- 66 #(Makanjoula et al., 2020)

NeL <- Ne*L_M

# This is the number of independent segment in the genome.
Me <- (2*NeL)/log10(NeL)


# Calculate Bonferroni threshold (already done)
bonf <- -log10(0.05 / Me)


setwd("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno")
# Read in and process data for Manhattan plot
yyy1 <- read.table("chrsnp_pval")
yyy <- yyy1[order(yyy1$V4), ]
zzz <- yyy[which(yyy$V1 == 1 & yyy$V2 == 3), ]
n <- nrow(zzz)
y <- zzz[, 4]
x <- zzz[, 3]
chr1 <- zzz[, 5]
chr <- NULL
pos <- NULL

for (i in unique(yyy$V5)) {
  zz <- yyy[yyy$V5 == i, ]
  key <- zz$V4
  medio <- round(nrow(zz) / 2, 0)
  z <- key[medio]
  pos <- c(pos, z)
}

chrn <- unique(yyy$V5)
one <- which(chr1 %% 4 == 0)
two <- which(chr1 %% 4 == 1)
three <- which(chr1 %% 4 == 2)
four <- which(chr1 %% 4 == 3)
chr[one] <- "darkgoldenrod"
chr[two] <- "darkorchid"
chr[three] <- "blue"
chr[four] <- "forestgreen"

# Create Manhattan plot with Bonferroni line and legend
png(file = "Pft1e3_manplot_with_bonf_ind_seg.png", width = 20000, height = 10000, res = 600)
par(mfrow = c(1, 1), family = "sans", cex = 1.5, font = 2)
plot(y, x, xaxt = "n", main = "Manhattan Plot SNP p_value - Trait: 1 Effect: 3", xlab = "", ylab = "-log10(p-value)", pch = 20, xlim = c(1, n), ylim = c(0, max(x)), col = chr)

# Add Bonferroni line
abline(h = bonf, col = "red", lwd = 2, lty = 2)

# Add legend for Bonferroni line
legend("topright", legend = "Bonferroni correction for genome independent segments", col = "red", lwd = 2, lty = 2, cex = 1)

axis(1, at = pos, labels = chrn)
dev.off()

```

<img src="image129.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">


#### Get rsID

For additional analysis like Variant Effect Prediction (VEP) we need the rsID, to get the rsID we use the software SNPChimp which requires SNP_names, but the BLUPF90 output have only the Chromosome and Position of the SNPs, so we are going to perfome these two steps to get one file with t he significant SNPs + SNP_name + rsID

##### Step 01 = Bring the SNP name to GWAS output

For this analysis we have to build this new sheet bringing SNP ID from snpmap.txt
```{r eval=FALSE}
gwas = read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/chrsnp_pval")
colnames(gwas) <- c("V1", "V2", "LOG_P", "SNP", "CHR", "BP")
gwas <- filter(gwas, LOG_P >= bonf)

snpmap <- read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/snpmap.txt", header = T)

# Filter snpmap to only include rows that match the CHR and BP values in out_genes
filtered_snpmap <- snpmap[snpmap$CHR %in% gwas$CHR & snpmap$POS %in% gwas$BP, ]

# Merge the filtered snpmap with out_genes
gwas_snpname <- merge(gwas, filtered_snpmap[, c("CHR", "POS", "SNP_ID")], 
                   by.x = c("CHR", "BP"), by.y = c("CHR", "POS"), 
                   all.x = TRUE)

gwas_snpname <- gwas_snpname[,c("CHR", "BP", "LOG_P", "SNP_ID")]

write.csv(gwas_snpname, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/chrsnp_pval_SNPid_ind_seg_sig_BLUPF90.csv")
```


```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/chrsnp_pval_SNPid_ind_seg_sig_BLUPF90.csv")

# Print the table using knitr::kable
knitr::kable(data)

```


##### Step 02 = Bring the rsID  to the file with SNP name.
After get the **rsID** from SNPCHIMP I produced this table with the code bellow:

```{r eval=FALSE}
rsid <- read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/SNPchimp_result_2150988331.tsv", header = T)

merged <- merge(rsid, gwas_snpname, by.x ="SNP_name", by.y ="SNP_ID")

colnames(merged)

merged <- merged[,c("SNP_name", "rs", "CHR", "BP", "LOG_P")]

colnames(merged) <- c("SNP_name", "rsID", "CHR", "BP", "LOG_P")

write.csv(merged, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/gwas_ind_seg_sig_SNPname_rsID.csv")

```

```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/gwas_ind_seg_sig_SNPname_rsID.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

### BLUPF90+ GALLO

```{r eval=FALSE}
# GALLO

#import a QTL annotation file
qtl_UCD1_2 <- import_gff_gtf(db_file="/home/bambrozi/2_CORTISOL/GALLO/Animal_QTLdb_release53_cattleARS_UCD1.gff.gz",file_type="gff")

#import a gene annotation file
gene_UDC1_2 <- import_gff_gtf(db_file="/home/bambrozi/2_CORTISOL/GALLO/Bos_taurus.ARS-UCD1.2.110.gtf.gz",file_type="gtf")

#import MARKER files = the GWAS output
gwas = read.csv("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/gwas_ind_seg_sig_SNPname_rsID.csv")
colnames(gwas) <- c("X", "SNP", "rsID", "CHR", "BP", "LOG_P")


#FINDING GENES AND QTLs ARROUND THE MARKER

#FINDING GENES
out.genes <- find_genes_qtls_around_markers(db_file= gene_UDC1_2, 
                                            marker_file= gwas, 
                                            method = "gene",
                                            marker = "snp", 
                                            interval = 50000, 
                                            nThreads = NULL)

write.csv(out.genes, file = "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/keep_all_pheno/out_genes_50k_pvalue.csv")

#FINDING QTLs

out.qtl <- find_genes_qtls_around_markers(db_file= qtl_UCD1_2, 
                                          marker_file= gwas, 
                                          method = "qtl",
                                          marker = "snp", 
                                          interval = 50000, 
                                          nThreads = NULL)


write.table(out.qtl, file = "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/keep_all_pheno/out_qtl_50k_pvalue.txt", 
            quote = FALSE, sep = "\t", row.names = FALSE, col.names = T)

library(tidyverse)
out.qtl.clean <- select(out.qtl, c("SNP", "rsID", "CHR", "QTL_type", "start_pos", "end_pos","QTL_ID"))
write.csv(out.qtl.clean, file = "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/keep_all_pheno/out_qtl_50k_pvalue_clean.csv")

```


The GALLO output are bellow:

**For GENES**
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/out_genes_50k_pvalue.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

**FOR QTLs**
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/out_qtl_50k_pvalue_clean.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

**QTL type**
<img src="image134.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**QTL name by type**
<img src="image197.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">


#### QTL enrichment on GALLO

```{r eval=FALSE}
#QTL enrichment analysis 
out.enrich_qtl_name <-qtl_enrich(qtl_db= qtl_UCD1_2, 
                                 qtl_file= out.qtl, qtl_type = "Name",
                                 enrich_type = "genome", chr.subset = NULL, 
                                 padj = "fdr",nThreads = 2)


# Sorting the dataframe in ascending order of adj.pval
sorted_df <- out.enrich_qtl_name[order(out.enrich_qtl_name$adj.pval), ]

write.csv(sorted_df,"/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/keep_all_pheno/out_enrich_qtl_genome_name.csv")

out.enrich_qtl_type <-qtl_enrich(qtl_db= qtl_UCD1_2, 
                                 qtl_file= out.qtl, qtl_type = "QTL_type",
                                 enrich_type = "genome", chr.subset = NULL, 
                                 padj = "fdr",nThreads = 2)

sorted_df_type <- out.enrich_qtl_type[order(out.enrich_qtl_type$adj.pval), ]
write.csv(out.enrich_qtl_type,"/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/keep_all_pheno/out_enrich_qtl_genome_type.csv")


#Plots

#Name

#Creating a new ID composed by the trait and the chromosome
out.enrich_qtl_name$ID<-paste(out.enrich_qtl_name$QTL," - ","CHR",out.enrich_qtl_name$CHR,sep="")

#Match the QTL classes and filtering the Reproduction related QTLs
out.enrich.filtered<-out.enrich_qtl_name[which(out.enrich_qtl_name$adj.pval<0.05),]

#Plotting the enrichment results for the QTL enrichment analysis
dev.off()
QTLenrich_plot(out.enrich.filtered, x="ID", pval="adj.pval")


#Type

#Creating a new ID composed by the trait and the chromosome
out.enrich_qtl_type$ID<-paste(out.enrich_qtl_type$QTL," - ","CHR",out.enrich_qtl_type$CHR,sep="")

#Match the QTL classes and filtering the Reproduction related QTLs
out.enrich.filtered_type<-out.enrich_qtl_type[which(out.enrich_qtl_type$adj.pval<0.05),]

#Plotting the enrichment results for the QTL enrichment analysis
dev.off()
QTLenrich_plot(out.enrich.filtered_type, x="ID", pval="adj.pval")

```


**QTL Enrichment outcomes**

**Enrichment by name** (enrichment analysis will be performed for each trait individually)
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/out_enrich_qtl_genome_name_pvalue.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

<img src="image132.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">  

**Enrichment by QTL_type** (enrichment processes performed for the QTL classes)
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/out_enrich_qtl_genome_type_pvalue.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

<img src="image133.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;"> 

#### BLUPF90+ GPROFILER ON-LINE

From the online version of GPROFILER i got the following results.

**Legend**
<img src="image41.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

<img src="image141.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

<img src="image142.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

#### BLUPF90+ VEP




## BLUPF90+ WINDOWS
### Running renf90_ssGWAS2_SNPeff_w.par for GWAS (WINDOWS)

The parameter card bellow we are going to run using the software **postGSf90**:

renf90_ssGWAS2_SNPeff_W_10.par
```{bash eval= FALSE}
# BLUPF90 parameter file created by RENUMF90
DATAFILE
renf90.dat
NUMBER_OF_TRAITS
           1
NUMBER_OF_EFFECTS
           3
OBSERVATION(S)
    1
WEIGHT(S)
 
EFFECTS: POSITIONS_IN_DATAFILE NUMBER_OF_LEVELS TYPE_OF_EFFECT[EFFECT NESTED]
 2         4 cross 
 3        22 cross 
 4      3724 cross 
RANDOM_RESIDUAL VALUES
   77182.    
 RANDOM_GROUP
     3
 RANDOM_TYPE
 add_an_upginb
 FILE
renadd03.ped                                                                    
(CO)VARIANCES
   28212.    
OPTION SNP_file bruno_gntps_iid_71_clean
OPTION origID
OPTION no_quality_control
OPTION readGInverse
OPTION readA22Inverse
OPTION map_file snpmap.txt_clean
OPTION snp_p_value
OPTION Manhattan_plot_R
OPTION Manhattan_plot
OPTION SNP_moving_average 10
OPTION windows_variance 10 

```

The parameter file above will generate the following files:
<ul>
<li> **snp_sol**
<ul>
<li> column 1: trait
<li> column 2: effect
<li> column 3: SNP
<li> column 4: Chromosome
<li> column 5: Position
<li> column 6: SNP solution
<li> column 7: weight
<li> column 8: variance explained by n adjacents SNP (if OPTION windows_variance is used).
<li> column 9: variance of the SNP solution (used to compute the p-value) (if OPTION snp_p_value is used)
</ul>
<li> **snp_pred**
<ul>
<li> contains allele frequencies + SNP effects
</ul>
<li> **chrsnpvar**
<ul>
<li> column 1: trait
<li> column 2: effect
<li> column 3: variance explained by n adjacents SNP
<li> column 4: SNP
<li> column 5: Chromosome
<li> column 6: Position
</ul>
<li> **chrsnp_pval**
<ul>
<li> column 1: trait
<li> column 2: effect
<li> column 3: -log10(p-value)
<li> column 4: SNP
<li> column 5: Chromosome
<li> column 6: Position in bp
</ul>
<li> **chrsnp**
<ul>
<li> column 1: trait
<li> column 2: effect
<li> column 3: values of SNP effects to use in Manhattan plots → [abs(SNP_i)/SD(SNP)]
<li> column 4: SNP
<li> column 5: Chromosome
<li> column 6: Position
</ul>
<li> **windows_segments**
<ul>
<li> column 1: label
<li> column 2: window size (number of SNP)
<li> column 3: Start SNP number for the window
<li> column 4: End SNP number for the window
<li> column 5: identification of window: (ChrNumber)'_'(startPositionMBP)
<li> column 6: Start (ChrNumber)'_'(Position) for the window
<li> column 7: End (ChrNumber)'_'(Position) for the window
</ul>
<li> **windows_variance**
<ul>
<li> column 1: trait
<li> column 2: effect
<li> column 3: Start SNP number or SNP name for the window
<li> column 4: End SNP number or SNP name for the window
<li> column 5: window size (number of SNP)
<li> column 6: Start (ChrNumber)'_'(Position) for the window
<li> column 7: End (ChrNumber)'_'(Position) for the window
<li> column 8: identification of window: (ChrNumber)'_'(startPositionMBP)
<li> column 9: variance explained by n adjacents SNP
</ul>
<li> **Vft1e3.R**
<li> **Sft1e3.R**
<li> **Pft1e3.R**
</ul>

Bellow we can see the SNPs that explain more than **0.5%** of Genetic Variance 

```{r eval=FALSE}
w_var <- read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/windows_10/chrsnpvar", header = F)
w_var <- filter(w_var, V3 > 0.5)
colnames(w_var) <- c("V1", "V2", "Var", "SNP", "CHR", "BP")
snp_map <- read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/windows_10/snpmap.txt_clean", header = T)

# Fazer o merge baseado em duas condições: CHR e POS
merged_data <- merge(w_var, snp_map, by.x = c("CHR", "BP"), by.y = c("CHR", "POS"), all.x = TRUE)
w_var <- merged_data[,c("CHR", "BP", "Var", "SNP_ID")]

write.csv(w_var, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/windows_10/snpID_w05.csv")

rsid <- read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/windows_10/SNPchimp_result_1425506748.tsv", header = T)


merged <- merge(rsid, w_var, by.x ="SNP_name", by.y ="SNP_ID")

colnames(merged)

merged <- merged[,c("SNP_name", "rs", "CHR", "BP", "Var")]

colnames(merged) <- c("SNP_name", "rsID", "CHR", "BP", "Var")

write.csv(merged, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/windows_10/snpID_w05_rsid_snpvar.csv")
```

```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/w10_snp_rsid_snpvar_05.csv")

# Print the table using knitr::kable
knitr::kable(data)

```



#### Manhattan Plots for BLUPF90 Windows ssGWAS

```{r eval=FALSE}
# Set working directory
setwd("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/windows_10/")
getwd()

# Read and process data
yyy1 = read.table("chrsnpvar")
yyy  = yyy1[order(yyy1$V4),]
zzz  = yyy[ which(yyy$V1==1 & yyy$V2==3), ]
n    = nrow(zzz)
y    = zzz[,4]
x    = zzz[,3]
chr1 = zzz[,5]
chr  = NULL
pos  = NULL
for (i in unique(yyy$V5)) {
  zz     = yyy[yyy$V5==i,]
  key    = zz$V4 
  medio  = round(nrow(zz)/2,0)
  z      = key[medio]
  pos    = c(pos,z)
}

# Assign colors for chromosomes
chrn       = unique(yyy$V5)
one        = which(chr1%%4==0) 
two        = which(chr1%%4==1) 
three      = which(chr1%%4==2) 
four       = which(chr1%%4==3) 
chr[one]   = "darkgoldenrod"
chr[two]   = "darkorchid"
chr[three] = "blue"
chr[four]  = "forestgreen"

# Generate Manhattan plot and save to PNG
png(file = "Vft1e3_manplot_with_thresholds.png", 
    width = 20000, height = 10000, res = 600) # Configure width, height, and resolution

# Set plot parameters
par(mfrow = c(1, 1), family = "sans", cex = 1.5, font = 2, mar = c(5, 5, 4, 2))

# Create Manhattan plot
plot(y, x, xaxt = "n", main = "Manhattan Plot SNP Variance explained by 10 adjacents SNP window", 
     xlab = "", ylab = "% variance expl", pch = 20, 
     xlim = c(1, n), ylim = c(0, max(x)), col = chr, cex.axis = 1.2)

# Add dashed lines for thresholds
abline(h = 0.5, col = "red", lwd = 2, lty = 2)  # Red dashed line at 0.5

# Add legend for thresholds
legend("topright", legend = c("Threshold 0.5%"), 
       col = "red", lwd = 2, lty = 2, cex = 1)

# Add chromosome labels on the X-axis
axis(1, at = pos, labels = chrn, las = 1, cex.axis = 0.8)

# Close the graphics device
dev.off()

```

<img src="image191.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">



### 0.5% Windows - BLUPF90+ GALLO

```{r eval=FALSE}
gwas <- read.csv("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/keep_all_pheno/windows_10/snpID_w05_rsid_snpvar.csv")
colnames(gwas)
colnames(gwas) <- c("X", "SNP", "rsID", "CHR", "BP", "Var")


# GALLO

#import a QTL annotation file
qtl_UCD1_2 <- import_gff_gtf(db_file="/home/bambrozi/2_CORTISOL/GALLO/Animal_QTLdb_release53_cattleARS_UCD1.gff.gz",file_type="gff")

#import a gene annotation file
gene_UDC1_2 <- import_gff_gtf(db_file="/home/bambrozi/2_CORTISOL/GALLO/Bos_taurus.ARS-UCD1.2.110.gtf.gz",file_type="gtf")

#FINDING GENES AND QTLs ARROUND THE MARKER

#FINDING GENES
out.genes <- find_genes_qtls_around_markers(db_file= gene_UDC1_2, 
                                            marker_file= gwas, 
                                            method = "gene",
                                            marker = "snp", 
                                            interval = 50000, 
                                            nThreads = NULL)

write.csv(out.genes, file = "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/keep_all_pheno/windows_10/out_genes_w_05.csv")

#FINDING QTLs

out.qtl <- find_genes_qtls_around_markers(db_file= qtl_UCD1_2, 
                                          marker_file= gwas, 
                                          method = "qtl",
                                          marker = "snp", 
                                          interval = 50000, 
                                          nThreads = NULL)


write.table(out.qtl, file = "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/keep_all_pheno/windows_10/out_qtl_w_05.txt", 
            quote = FALSE, sep = "\t", row.names = FALSE, col.names = T)

library(tidyverse)
out.qtl.clean <- select(out.qtl, c("SNP", "rsID", "CHR", "QTL_type", "start_pos", "end_pos","QTL_ID"))
write.csv(out.qtl.clean, file = "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/keep_all_pheno/windows_10/out_qtl_w_05_clean.csv")
```

The GALLO output are bellow:

**For GENES**
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/out_genes_w_05.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

**FOR QTLs**
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/out_qtl_w_05_clean.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

**QTL Plots**

```{r eval=FALSE}
# Set working directory
setwd("/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/windows10")

#QTL type plot
oldpar <- par(mar=c(1,15,0.5,1))
plot_qtl_info(out.qtl, qtl_plot = "qtl_type", cex=1.5)

#QTL names plot (by type)

# Set the width and height in pixels for 600 DPI
png("qtl_names_w05.png", width=5100, height=6600, res=600)  # Width and height in pixels
# Set layout for multiple plots
par(mfrow=c(6, 1), mar=c(2, 20, 1, 1))
# List of QTL classes for titles
qtl_classes <- c("Production", "Reproduction", "Milk", "Meat_and_Carcass", "Health", "Exterior")
# Loop through each QTL class and plot
for (qtl_class in qtl_classes) {
  plot_qtl_info(out.qtl, qtl_plot = "qtl_name", qtl_class=qtl_class)
  title(main=qtl_class)  # Add the QTL class as the main title for each plot
}
# Close the device
dev.off()


#QTL enrichment analysis 

out.enrich_qtl_name <-qtl_enrich(qtl_db= qtl_UCD1_2, 
                                 qtl_file= out.qtl, qtl_type = "Name",
                                 enrich_type = "genome", chr.subset = NULL, 
                                 padj = "fdr",nThreads = 2)


# Sorting the dataframe in ascending order of adj.pval
sorted_df <- out.enrich_qtl_name[order(out.enrich_qtl_name$adj.pval), ]
write.csv(sorted_df,"/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/windows10/out_enrich_qtl_genome_name_w05.csv")


out.enrich_qtl_type <-qtl_enrich(qtl_db= qtl_UCD1_2, 
                                 qtl_file= out.qtl, qtl_type = "QTL_type",
                                 enrich_type = "genome", chr.subset = NULL, 
                                 padj = "fdr",nThreads = 2)

sorted_df_type <- out.enrich_qtl_type[order(out.enrich_qtl_type$adj.pval), ]
write.csv(out.enrich_qtl_type,"/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/windows10/out_enrich_qtl_genome_type_w05.csv")


#Plots

#Name
#Creating a new ID composed by the trait and the chromosome
out.enrich_qtl_name$ID<-paste(out.enrich_qtl_name$QTL, out.enrich_qtl_name$CHR,sep="")
#Match the QTL classes and filtering the Reproduction related QTLs
out.enrich.filtered<-out.enrich_qtl_name[which(out.enrich_qtl_name$adj.pval<0.05),]
#Plotting the enrichment results for the QTL enrichment analysis
dev.off()
QTLenrich_plot(out.enrich.filtered, x="ID", pval="adj.pval")


#Type
#Creating a new ID composed by the trait and the chromosome
out.enrich_qtl_type$ID<-paste(out.enrich_qtl_type$QTL, out.enrich_qtl_type$CHR,sep="")
#Match the QTL classes and filtering the Reproduction related QTLs
out.enrich.filtered_type<-out.enrich_qtl_type[which(out.enrich_qtl_type$adj.pval<0.05),]
#Plotting the enrichment results for the QTL enrichment analysis
dev.off()
QTLenrich_plot(out.enrich.filtered_type, x="ID", pval="adj.pval")
```


**QTL type**
<img src="image182.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**QTL name by type**
<img src="image192.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

#### Windows - QTL enrichment on GALLO

**QTL Enrichment outcomes**

**Enrichment by name** (enrichment analysis will be performed for each trait individually)
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/out_enrich_qtl_genome_name_w05.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

<img src="image189.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">  

**Enrichment by QTL_type** (enrichment processes performed for the QTL classes)
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/out_enrich_qtl_genome_type_w05.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

<img src="image190.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;"> 



## Comparison among ssGWAS for independent SNPs against Window 0.5% and window 0.1%

### SNPs

```{r eval=FALSE}
pvalue <- read.csv("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/gwas_ind_seg_sig_SNPname_rsID.csv")
w05 <- read.csv("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/window_10/w10_snp_rsid_snpvar_05.csv")
w01 <- read.csv("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/window_10/w10_snp_rsid_snpvar_01.csv")



intersecpt_pv_w05_name <- intersect(pvalue$SNP_name, w05$SNP_name)
intersecpt_pv_w01_name <- intersect(pvalue$SNP_name, w01$SNP_ID)


# Create the matrix
common_ids <- matrix(NA, nrow = length(intersecpt_pv_w01_name), ncol = 3)
common_ids[, 1] <- intersecpt_pv_w01_name  # First column
common_ids[, 2] <- ifelse(intersecpt_pv_w01_name %in% intersecpt_pv_w01_name, "YES", NA)
common_ids[, 3] <- ifelse(intersecpt_pv_w01_name %in% intersecpt_pv_w05_name, "YES", NA)
common_ids <- as.data.frame(common_ids)
colnames(common_ids) <- c("P_value_SNPs", "W_0.1%_SNPs", "W_0.5%_SNPs")
common_ids$rsID <- pvalue$rsID[match(common_ids$P_value_SNPs, pvalue$SNP_name)]
common_ids <- common_ids[, c("P_value_SNPs", "rsID", "W_0.1%_SNPs", "W_0.5%_SNPs")]

write_csv(common_ids, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/common_SNPs.csv")
```

Bellow are the SNPs which had the p-value bellow bonferroni and also apeeared in the set of SNPs which explain more thant 0.5% or 0.1% of genetic variance.

```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/common_SNPs.csv")

# Print the table using knitr::kable
knitr::kable(data)

```


### Genes

```{r eval=FALSE}
pvalue <- read.csv("/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/out_genes_50k_pvalue.csv")
w05 <- read.csv("/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/windows10/out_genes_w_05.csv")
w01 <- read.csv("/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/windows10/out_genes_w01.csv")

intersecpt_pv_w05_name <- intersect(pvalue$gene_name, w05$gene_name)
intersecpt_pv_w05_ID <- intersect(pvalue$gene_id, w05$gene_id)

intersecpt_pv_w01_name <- intersect(pvalue$gene_name, w01$gene_name)
intersecpt_pv_w01_ID <- intersect(pvalue$gene_id, w01$gene_id)

# Create the matrix
common_ids <- matrix(NA, nrow = length(intersecpt_pv_w01_ID), ncol = 3)
common_ids[, 1] <- intersecpt_pv_w01_ID  # First column
common_ids[, 2] <- ifelse(intersecpt_pv_w01_ID %in% intersecpt_pv_w01_ID, "YES", NA)
common_ids[, 3] <- ifelse(intersecpt_pv_w01_ID %in% intersecpt_pv_w05_ID, "YES", NA)
common_ids <- as.data.frame(common_ids)
colnames(common_ids) <- c("P_value_id", "W_0.1%", "W_0.5%")
common_ids$P_value_name <- pvalue$gene_name[match(common_ids$P_value_id, pvalue$gene_id)]
common_ids <- common_ids[, c("P_value_id", "P_value_name", "W_0.1%", "W_0.5%")]
common_ids$gene_biotype <- pvalue$gene_biotype[match(common_ids$P_value_id, pvalue$gene_id)]

write_csv(common_ids, "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/common_genes.csv")

```

Bellow we can find the Genes that were found close (50kb) to SNPs that were:
<ul>
<li> Individually significant (P_value)
<li> Explain more than 0.5% additive genetic variance
<li> Explain more than 0.1% additive genetic variance
</ul>

```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/common_genes.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

### QTLs

```{r eval=FALSE}
# GALLO

#import a QTL annotation file
qtl_UCD1_2 <- import_gff_gtf(db_file="/home/bambrozi/2_CORTISOL/GALLO/Animal_QTLdb_release53_cattleARS_UCD1.gff.gz",file_type="gff")


#import MARKER files = the GWAS output
gwas_p = read.csv("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/gwas_ind_seg_sig_SNPname_rsID.csv")
colnames(gwas_p) <- c("X", "SNP", "rsID", "CHR", "BP", "LOG_P")

pvalue <- find_genes_qtls_around_markers(db_file= qtl_UCD1_2, 
                                         marker_file= gwas_p, 
                                         method = "qtl",
                                         marker = "snp", 
                                         interval = 50000, 
                                         nThreads = NULL)

gwas_w05 <- read.csv("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/window_10/w10_snp_rsid_snpvar_05.csv")
colnames(gwas_w05) <- c("X", "SNP", "rsID", "CHR", "BP", "Var")

w05 <- find_genes_qtls_around_markers(db_file= qtl_UCD1_2, 
                                         marker_file= gwas_w05, 
                                         method = "qtl",
                                         marker = "snp", 
                                         interval = 50000, 
                                         nThreads = NULL)

gwas_w01 <- read.csv("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/window_10/w10_snp_rsid_snpvar_01.csv")
colnames(gwas_w01)
colnames(gwas_w01) <- c("X", "CHR", "BP", "SNP", "Var")
gwas_w01 <- gwas_w01[, c("X","SNP", "CHR", "BP", "Var")]

w01 <- find_genes_qtls_around_markers(db_file= qtl_UCD1_2, 
                                          marker_file= gwas_w01, 
                                          method = "qtl",
                                          marker = "snp", 
                                          interval = 50000, 
                                          nThreads = NULL)



#################################################################

intersecpt_pv_w05_ID <- intersect(pvalue$QTL_ID, w05$QTL_ID)

intersecpt_pv_w01_ID <- intersect(pvalue$QTL_ID, w01$QTL_ID)

# Create the matrix
common_ids <- matrix(NA, nrow = length(intersecpt_pv_w01_ID), ncol = 3)
common_ids[, 1] <- intersecpt_pv_w01_ID  # First column
common_ids[, 2] <- ifelse(intersecpt_pv_w01_ID %in% intersecpt_pv_w01_ID, "YES", NA)
common_ids[, 3] <- ifelse(intersecpt_pv_w01_ID %in% intersecpt_pv_w05_ID, "YES", NA)
common_ids <- as.data.frame(common_ids)
colnames(common_ids) <- c("P_value_id", "W_0.1%", "W_0.5%")
common_ids$P_value_name <- pvalue$Name[match(common_ids$P_value_id, pvalue$QTL_ID)]
common_ids <- common_ids[, c("P_value_id", "P_value_name", "W_0.1%", "W_0.5%")]
common_ids$QTL_type <- pvalue$QTL_type[match(common_ids$P_value_id, pvalue$QTL_ID)]

write_csv(common_ids, "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/common_QTLs.csv")

```

Bellow we can find QTLs that were found close (50kb) to SNPs that were:
<ul>
<li> Individually significant (P_value)
<li> Explain more than 0.5% additive genetic variance
<li> Explain more than 0.1% additive genetic variance
</ul>

```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/common_QTLs.csv")

# Print the table using knitr::kable
knitr::kable(data)

```


## Allele frequency for GWAS output

I created the code bellow to find out the allele frequency for those most significant SNPs considering the two different methodologies, the "regular"ssGWAS and the window ssGWAS.

```{r eval=FALSE}

# Bringing SNP_ID to SNP_Frequency
snpmap <- read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/window_10/snpmap.txt_clean", header=T)
snpfreq <- read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/window_10/freqdata.count")
map_freq_var <- cbind(snpmap, snpfreq,snpWvar)
colnames(map_freq_var) <- c("CHR", "POS", "SNP_ID", "SNP_ORDER", "FREQ", "V1", "V2", "Var", "snp", "chr", "pos")
map_freq_var <- map_freq_var[,c("CHR", "POS", "SNP_ORDER", "SNP_ID", "FREQ", "Var") ]
map_freq_var_w05 <- filter(map_freq_var, Var > 0.5)
map_freq_var_w01 <- filter(map_freq_var, Var > 0.1)
write.csv(map_freq_var_w05, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/window_10/w05_allele_freq.csv")
write.csv(map_freq_var_w01, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/window_10/w01_allele_freq.csv")



# Output for significant p-value

#Estimating Bonferroni for genome independent segments
Genome_Assembly_ARS_UCD_1_2 <- read_tsv("/home/bambrozi/2_CORTISOL/GWAS/sequence_report_ARS-UCD1_2.tsv")

library(dplyr)
# Filter the rows and sum the Seq length column
# Assuming your data frame is named Genome_Assembly_ARS_UCD_1_2
L <- Genome_Assembly_ARS_UCD_1_2 %>%
  filter(`UCSC style name` %in% paste0("chr", 1:29)) %>%
  summarise(total_length = sum(`Seq length`)) %>%
  pull(total_length)
# Converting bases to Morgan (1Mb = 1cM (0,01 Morgan))
L_M <- L/10^8
# The Ne measure is based on the article bellow:
Ne <- 66 #(Makanjoula et al., 2020)
NeL <- Ne*L_M
# This is the number of independent segment in the genome.
Me <- (2*NeL)/log10(NeL)
# Calculate Bonferroni threshold (already done)
bonf <- -log10(0.05 / Me)

library(tidyverse)
#import MARKER files = the GWAS output
snpPval = read.table("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/chrsnp_pval")
colnames(snpPval) <- c("V1", "V2", "LOG_P", "SNP", "CHR", "BP")
map_freq_logp <- cbind(snpmap, snpfreq,snpPval)
map_freq_logp <- map_freq_logp[, c("CHR", "POS", "SNP", "SNP_ID", "V2", "LOG_P")]
colnames(map_freq_logp) <- c("CHR", "POS", "SNP", "SNP_ID", "FREQ", "LOG_P")
map_freq_logp <- filter(map_freq_logp, LOG_P >= bonf)
write.csv(map_freq_logp, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/pval_allele_freq.csv")


common_snp <- read.csv("/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/common_SNPs.csv")
common_snp$allele_freq <- map_freq_logp$FREQ[match(common_snp$P_value_SNPs, map_freq_logp$SNP_ID)]
write.csv(map_freq_logp, "/home/bambrozi/2_CORTISOL/GWAS/BLUPF90/EXTREME_PHENO/common_SNPs_FREQ.csv")

```

### "Regular" ssGWAS

```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/pval_allele_freq.csv")

# Print the table using knitr::kable
knitr::kable(data)

```


### Windows ssGWAS

#### 0.5%
```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/w05_allele_freq.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

#### 0.1%
As this table has 1,089 rows we are not showing here, but we are going to show bellow the allele frequency of the **common SNPs among p-value, Windows 0.5% and Windows 0.1% approach**:


```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/common_SNPs_FREQ.csv")

# Print the table using knitr::kable
knitr::kable(data)

```


# Heritability estimation - BLUPF90

## Files preparation

Preparing files to run Variance components estimation using REML with AI (Average Information) algorithm.

First you need to create a directory in your home directory, prepare and save the following files in:

<ul>
<li> Phenotype and Fixed effects file
<li> Pedigree file
<li> Genotype file
<li> BlupF90+ executable file
<li> RenumF90 executable file
<li> Parameter file
<ul>


### Phenotype and Fixed effects file

The appearance of this file is like this:

<img src="image95.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**FIRST COLUMN** = Animal ID
**SECOND COLUMN** = Phenotype
**THIRD COLUMN** = Fixed Effect 1
**FOURTH COLUMN** = Fixed Effect 2

To get in one file these four columns we need the following code:

```{r eval=FALSE}

#File with equivalence among different ids
eq_ids <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/Pedigree/bruno_ids.csv")

# Genotype file with cid
geno <- read.table("/home/bambrozi/2_CORTISOL/Geno_files/genoplink.ped")

# Phenotipic file and fixed effects
data_final <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/data_GEBVs_Cortisol_select_traits2.csv")

# creating a pheno file with only ID, Cortisol, BY and Sam date columns
pheno <- data_final %>%
  select(ID, T4Cortisol, BIRTH_YEAR, Sampling_date)

# Create a new column iid and and bring the iid from eq_ids to geno file
pheno$iid <- eq_ids$iid[match(pheno$ID, eq_ids$elora_id)]

# organizing columns sequence and keep only iid
pheno <- pheno%>%
  select(iid, T4Cortisol, BIRTH_YEAR, Sampling_date)

# Create a new column geno$iid, and bring the iid from eq_ids to geno file
geno$iid <- eq_ids$iid[match(geno$V2, eq_ids$cdn_id)]

# organizing the columns sequence
library(dplyr)
geno <- geno %>%
  select(V1, V2, iid, everything())

# Keeping in the pheno file only the rows present also in geno file
pheno <- pheno %>%
  filter(iid %in% geno$iid)

write.table(pheno, "/home/bambrozi/2_CORTISOL/Heritability_BLUPF90/pheno_fix_eff.txt", sep = " ", col.names = FALSE, row.names = FALSE, quote = FALSE)

```


The file should be saved as text file, with separation by space and no columns names.

### Pedigree file

The appearance of this file is like this:

<img src="image96.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**FIRST COLUMN** = Animal ID
**SECOND COLUMN** = Sire ID
**THIRD COLUMN** = Dam ID

The file should be saved as text file, with separation by space and no columns names.

We used the code below to remove the commas of a .csv file to a file with sepation by spaces.
```{bash eval=FALSE}
# to replace comma for space in the .csv file with the equivalence among IDs
sed -i 's/,/ /g' bruno_ids.csv
```

### Genotype file

The appearance of this file is like this:

<img src="image97.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

**FIRST COLUMN** = Animal ID
**SECOND COLUMN** = Genotypes (0, 1 and 2 format)

The file should be saved as text file, with separation by space and no columns names.

We  used the code below to replace the cid for iid. First we merge using the second column of the firs file, and the first column of the second file. Then we use again the command awk to keep only the third and fifth columsn and sabe in a different object.


```{bash eval=FALSE}
# Using the awk function to merge the two files and the second awk to select only the 3rd and 5fh columns
awk 'FNR==NR {a[$2]=$0; next} {print a[$1], $0}' bruno_ids.csv bruno_gntps.txt | awk '{print$3,$5}' > bruno_gntps_iid
```

### Download the executable files

Download from this website <https://nce.ads.uga.edu/html/projects/programs/Linux/64bit/>:
<ul>
<li> BlupF90+
<li> renumF90
<ul>

### Parameter file

The appearance of this file is like this:

<img src="image98.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

<ul>
<li> **DATAFILE**: bellow this line you need to inform the name of the file with phenotype and fixed effects. As before running BLUPF90 on server you are going to direct the terminal to the directory where all these files are placed you only need to inform the name.
<li> **TRAITS**: below this line you need to inform which column are the phenotype date in the previous file, in this example, 2.
<li> **FIELDS_PASSED TO OUTPUT**:
<li> **WEIGHT (S)**:
<li> **RESIDUAL VARIANCE**: for the firs run you need to inform the value of 1.0, for the second you can pick the variance from the firs run's output.
<li> **EFFECT**: you will inform your first effect, in this example, Birth Year, which is in the column 3, and the word **cross numer** because is a number.
<li> **EFFECT**: you should provide the next effect, in this example, sample date, as sample date has one non numeric character you should inform as **cross alpha**, in this example column 4.
<li> **EFFECT**: now I'm providing my animal ID information, in this example column 1, and again **cross alpha** because has number and letters in the ID. I'm also informing that this effect is **RANDOM**, and that is my **animal** effect.
<li> **FILE**: bellow this line I need to provide the pedigree file. Again, as I'm already in the directory which contain the pedigree file I only need to provide the file name.
<li> **FILE-POS**: Here I'll inform which columns should be considered in the pedigree file, in this situation, **1 2 3 0 0**.
<li> **PED_DEPTH**: Now we can inform the depth we want the software considers the pedigree, or if we leave **0** it will the maximum possible.
<li> **(CO) VARIANCES**: Here you should provide the Variance/Co-variance matrix, like as for residual variance in the first run we set up to 0 in this example that we don´t have to imagine any co-variance, but if you know that exist variance among you effects you shoul set up **XXX** for ....
<li> **OPTION method**: VCE (Variance Component Estimation).
<li> **OPTION OrigID**: this will keep the original ID informed.
<li> **OPTION missing 9999**: you are informing that missing values will appear as **9999**
<li> **OPTION se_covar_function**: H2_1 g_3_3_1_1/(g_3_3_1_1+r_1_1)
<ul> 
<li> **H2_1**: the name that your function will appear on the output files.
<li> **g_3_3**: you are asking for genetic variance estimation for the 3rd informed effect.
<li> **_1_1**: this effect is in the 1st column. 
<li> **/(g_3_3_1_1+r_1_1)**: to get the total phenotipic variance, you are summing to genetic variance the residual variance of the effect in column 1.
<ul>
<ul>

## Running renumF90 and BlupF90+

1. Go to the server you wanna run this analysis, for instance, **grand**

2. Now go to the directory you have created to run this analysis where that set of files are placed.
```{bash eval=F}
ssh grand
```

3. Make the renumF90 and BlupF90+ files executables
```{bash eval=F}
chmod +x renumf90
chmod +x blupf90+
```

4. Run renumF90
```{bash eval=F}
./renumf90
```
When you run the code above, it will as you the name of your parameter card.

renumF90 will generate a new parameter card called **renf90.par**

5. Run blupf90+
```{bash eval=F}
./blupf90+
```
blupf90+ will ask you for parameter's card name, now you should provide with the new one **renf90.par**

blupF90+ will generate the **blupf90.log** file with the results.

<img src="image99.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

Now you should update you **renf90.par** file with these informations from the .log file

Copy **Residual Variance** from blupf90.log and will paste on renf90.par **RANDOM_RESIDUAL_VALUES**
Copy **Genetic variance for effect x** from blupf90.log and will paste on renf90.par **(CO) VARIANCE**

6. 2nd blupf90+ run
```{bash eval=F}
./blupf90+
```
blupf90+ will ask you for parameter's card name, now you should provide with the UPDATED **renf90.par**

If the **Residual Variance** and **Genetic variance for effect x** didn't change in your blupf90.log the analysis ended, but if this value vary, you should update again the renf90.par and run again blupf90+ until this values don't change more.


## Running renumF90 and BlupF90+ adding GENOTYPES

The previous analysis considered only the pedigree, but now we can insert the genotype information. To perform this you need a new diretory called **Blup_Genomic** inside your previously created directory.

Now you need add the reference for your genotype file in your previous parameter file **renum.par** and save in this new sub-directory. 

The <span style="background-color: yellow;">highlighted</span> text show the added part.

<img src="image100.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

Go to the sub-diretory

Note that as you are in the subdirectory, but your phenotype and fixed effect, pedigree and genotype files are still in the previous directory you need to add the  <span style="background-color: lightblue;">highlighted</span> part to inform the correct location

<img src="image101.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

To run the renumf90 and blupf90+ you also need to add **../** to correct specify the location.
Run renumF90
```{bash eval=F}
./../renumf90
```

Run blupf90+
```{bash eval=F}
./../blupf90+
```

The steps for run, update parameter card, re-run are the same.

## Results

We have 2 different output files

1) Variance components: blupf90.log  

<img src="image102.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">

In this file we can find the heritabilit (SD) and for instance the convergence (similarity)

2) Solutions: solutions.orig  
In this file we will find the solutions (results) for each effect

In our example:

<ul>
<li> **EFFECT 1**: Birth Year, has 4 levels (2018, 2019, 2020 and 2021), and the solution that for this fixed effect is how much each level add.
<li> **EFFECT 2**: Sampling date, has 23 levels, and the solutions
<li> **EFFECT 3**: Animal random effect, has one for each animal and it is the EBV or GEBV.



<img src="image103.png" alt="My Image" style="display: block; margin-left: auto; margin-right: auto;">


## Additional analysis

### Age at sampling date
```{r eval=FALSE}
Merg_Cort_GEBVs <- read.csv("/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/Merged_Cortisol_GEBVs.csv")

#Opening the file with the GEBVs columns to use
Columns_to_use <- readLines("/home/bambrozi/2_CORTISOL/RawFiles/GEBVs_Elora/traits_to_use.txt")

colnames(Merg_Cort_GEBVs)[405] <- "IDD"

data <- select(Merg_Cort_GEBVs, ID, T4Cortisol, BIRTH_YEAR, BIRTH_DATE, all_of(Columns_to_use))

samp_date2 <- read.csv("/home/bambrozi/2_CORTISOL/Data/Elora animal_ids_kl_sampling_date.csv")

# Convert Sampling_date to Date using as.Date
samp_date2$Sampling_date <- as.Date(samp_date2$Sampling_date, format = "%m/%d/%Y")

table(samp_date2$Sampling_date)

samp_date <- select(samp_date2, Elora_id, Sampling_date)

# Check if data$ID and samp_dates$elora_id are identical in values and order
identical(data$ID, samp_date$Elora_id)

data_final <- merge(data, samp_date, by.x="ID", by.y="Elora_id")

data_final <- data_final %>%
  select(ID, T4Cortisol, BIRTH_YEAR, BIRTH_DATE, Sampling_date)

library(lubridate)

data_final$BIRTH_DATE <- ymd(data_final$BIRTH_DATE)

# Calculate age in total days
data_final$Age_days <- as.numeric(difftime(data_final$Sampling_date, data_final$BIRTH_DATE, units = "days"))


# The data below has the the 55 GEBVs + Cortisol data + Birth Year + Sampling data
write.csv(data_final, "/home/bambrozi/2_CORTISOL/Data/data_age_samplingdate.csv")

# Calculate values
average_age <- mean(data_final$Age_days, na.rm = TRUE)
sd_age <- sd(data_final$Age_days, na.rm = TRUE)
median_age <- median(data_final$Age_days, na.rm = TRUE)
range_age <- range(data_final$Age_days, na.rm = TRUE)

# Create a data frame to save
age_summary <- data.frame(
  Metric = c("Mean", "Standard Deviation", "Median", "Min", "Max"),
  Value = c(average_age, sd_age, median_age, range_age[1], range_age[2])
)

# Save to a CSV file
write.csv(age_summary, "/home/bambrozi/2_CORTISOL/Data/age_summary.csv", row.names = FALSE)

```

```{r echo=FALSE}
# Read the CSV file into a data frame
data <- read.csv("C:/Users/galin/OneDrive - University of Guelph/0.Post doc/4_Cortisol/Git_Cortisol/age_summary.csv")

# Print the table using knitr::kable
knitr::kable(data)

```

### Common enriched QTLs

```{r eval=FALSE}
qtl_pval <- read.csv("/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/out_enrich_qtl_genome_name_pvalue.csv")
qtl_w05 <- read.csv("/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/windows10/out_enrich_qtl_genome_name_w05.csv")
qtl_w01 <- read.csv("/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/windows10/out_enrich_qtl_genome_name_w01.csv")

library(tidyverse)

qtl_pval <- filter(qtl_pval, adj.pval <= 0.05) 
qtl_w05 <- filter(qtl_w05, adj.pval <= 0.05) 
qtl_w01 <- filter(qtl_w01, adj.pval <= 0.05) 

qtl_pval$approach <- "pval"
qtl_w05$approach <- "w05"
qtl_w01$approach <- "w01"

qtl_all_approachs <- rbind(qtl_pval, qtl_w05, qtl_w01)

write.csv(qtl_all_approachs, "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/enrich_sig_qtl_all_approachs.csv")

qtl_names <- unique(qtl_all_approachs$QTL)

qtl_names <- sort(qtl_names)


# Create a matrix with the desired dimensions
matrix_length <- length(qtl_names)
common_enrich_qtl <- matrix(NA, nrow = matrix_length, ncol = 4)

# Optionally, name the rows and columns
colnames(common_enrich_qtl) <- c("QTL_names", "Single_SNP", "Windows_0.5", "Windows_0.1")

common_enrich_qtl[, "QTL_names"] <- qtl_names

common_enrich_qtl <- as.data.frame(common_enrich_qtl)

# Ensure both vectors have the same length (or handle if they don't)
common_enrich_qtl$Single_SNP <- ifelse(common_enrich_qtl$QTL_names %in% qtl_pval$QTL, "yes", NA)
common_enrich_qtl$Windows_0.5 <- ifelse(common_enrich_qtl$QTL_names %in% qtl_w05$QTL, "yes", NA)
common_enrich_qtl$Windows_0.1 <- ifelse(common_enrich_qtl$QTL_names %in% qtl_w01$QTL, "yes", NA)

write.csv(common_enrich_qtl, "/home/bambrozi/2_CORTISOL/GALLO/GWAS_BLUPF90/common_enrich_qtl.csv")
```



